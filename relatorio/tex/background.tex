\chapter{Background}\label{chapter:background}

This section provides an overview of the essential concepts and foundational knowledge necessary for understanding the methods and approaches used throughout this thesis. A solid understanding of these concepts is necessary to fully comprehend the work presented. {\color{blue}The section begins with a discussion on benchmarking, inlcuding the best practices for performing benchmarks. It then explores energy profiling, which involves benchmarking but focused on energy measurements. It explains the energy tools capable of performing the energy measurements, how they work, what are the advantages and disadvantages. Additionally, it introduces the concepts of static and dynamic analysis explaining the differences and each is applied. The Language Server Protocol concept is also examined as it has relevance in the context of this thesis. Finally, it has a brief explanation on machine learning and how it can be used in our project.}


\section{Benchmarking} \label{sec:background_benchmarking}

Benchmarking is the process of measuring the performance of a program or system by running a series of tests under controlled conditions. This typically involves evaluating factors such as execution time, memory usage, and energy consumption. The purpose of benchmarking is to gain a clear understanding of how the software behaves in different scenarios, to compare different implementations, and to identify potential areas for improvement.

In this project, the primary goal of benchmarking is to accurately measure the energy consumption of Java applications, while taking into account the factors that can affect these measurements. \wo{aqui voltas a falar dos programas mas desta vez na forma de benchmarks. Me parecem mesmo que estamos a lidar com um gerador de benchmarks e não de programas}

When performing benchmarking (i.e. measuring time, energy, memory, etc.) it is important to have in mind some important information that can affect the measurements. First it matters to know how the programming language being used works, and how it can affect the measurements. It is important to understand which noises can be avoided and which cannot.

As an example, benchmarks in Java, which is the primary programming language used in this project, will have its constraints. Performing benchmarks in Java has its adversities, and needs careful approach when needed. Several papers have studied how to perform better benchmarks~\cite{10.1145/1297027.1297033,10.1145/1167515.1167488} details important aspects on how to correctly benchmark Java applications, and how to avoid common pitfalls.
It is essential to understand the non-deterministic problems that may arrive when benchmarking in Java:


\begin{itemize}
    \item \textbf{Just-In-Time (JIT) compilation}: Automatically performs optimizations in during the compilation, which can affect measurements. Example: In the loop presented in the Listing~\ref{lst:for_loop_example}, the optimization done during the compilation, might understand that the loop is unnecessary and just replace the variable sum with \texttt{int sum = 100;}, avoiding the loop. Basically, the JIT compiler may detect that a loop performs predictable work and optimize it away entirely, especially if the results are unused or can be computed ahead of time.
    
    \begin{listing}[H]
    \noindent\rule{\linewidth}{0.4pt}
    \begin{minted}[linenos, fontsize=\small, frame=none, bgcolor=white,breaklines=true,breakanywhere=true]{Java}
        int sum = 0;
        for (int i = 0; i < 100; i++) {
            sum++;
        }
        System.out.println(sum);
    \end{minted}
    \noindent\rule{\linewidth}{0.4pt}
    \caption{Example for loop that can be optimized by the JIT compiler.}            
    \label{lst:for_loop_example}
    \end{listing}

    
    \item \textbf{Garbage Collection (GC)}: Garbage collection can occur unpredictably, which may impact the accuracy of measurements. For time and energy measurements, it often results in increased values, while for memory measurements, the effect depends on whether garbage collection occurs before or after the measurement period.
    
    \item \textbf{Thread scheduling}: It is also unpredictable when threads stop might have different schedules, resulting in different interactions.
    
    \item \textbf{Java Virtual Machine (JVM)}: Different JVM implementations or configurations (e.g., heap size, GC algorithms, JIT strategies) can result in considerable performance differences. Even the same JVM may behave differently across runs due to internal optimizations and adaptive behaviors. Additionally, JVM startup time can be a problem in performance measurements, especially in short processes, as it introduces overhead not representative of steady-state behavior.

\end{itemize}

Some of these properties cannot be avoided, however there are some procedures that can be taken into consideration to avoid error in the measurements.
To avoid JIT optimizations, harder and complex examples must be used in order to avoid optimizations, for example instead of filling a list in a for loop using the index values, the values added can be random, which will avoid optimizations, as the compiler cannot predict the next number in the iteration. This proves to be effective in energy measurement, because if it is needed to measure the energy consumption of the add method of a list collection, now the process can actually be measured avoiding optimizations, if it was optimized it would be too fast and would not be valid for energy measurement.
The JVM start up can be avoided by starting the program without measuring anything and just start the measurement when the computation method is ready to be started and not for the whole machine. Avoiding reading JVM startup, warm up, even unnecessary computations, and focusing only on the necessary parts.
Garbage collector can be harder, while \texttt{System.gc()} can be used to suggest a garbage collection cycle, the JVM is free to ignore this request. As such, it is not a reliable mechanism to control GC timing. An alternative is to increase the memory the JVM can use, so the GC is called fewer times. 


It is also important to note that:


\begin{itemize}
    \item \textbf{Use multiple JVM invocations}: Run benchmarks in new JVM instances to capture variations and prevent biased warm-up effects.
    
    \item \textbf{Use multiple JVMs and versions}: Different JVM implementations and versions may apply different optimizations, affecting performance outcomes.
    
    \item \textbf{Avoid cherry-picking values}:  Instead of reporting only the best or worst results, use the median or mean along with variability.
    
    \item \textbf{Vary hardware configurations}: Using different hardware will have different results.
    
    \item \textbf{Test with multiple heap sizes}: Garbage collection behavior and performance can change depending on the heap size. Test different benchmarks with a range of heap sizes, starting from the minimum required.
    
    \item \textbf{Use realistic, diverse workloads}: Use real-world applications instead of microbenchmarks to better reflect practical behavior.
    
    \item \textbf{Avoid background system noise}: When running the benchmarks, preferably only have the necessary processes running, to reduce measurement noise.
\end{itemize}

When building the tool, it is necessary to gather data from the Java programs, which requires benchmarking best practices. Although it is not possible to apply all of them due to time and complexity constraints, it is always helpful to keep in mind what it takes to create a good benchmark. In this project, the techniques applied included using multiple JVM invocations, avoiding cherry-picking by reporting average values, minimizing background system noise, excluding JVM startup time, and focusing measurements on the relevant computation phases.



\section{Energy Profiling} \label{sec:background_energy_profiling}

To fully understand the processes involved in this work, it is important to first understand what energy profiling is, as it will be frequently referenced and used later.

Energy profiling is the systematic process of measuring, monitoring, and analyzing the power consumption of a system, using specialized software or hardware tools to collect detailed power consumption data. This data can be collected from different parts of a system, including applications, processes, and specific snippets of code, to provide insight into how different components and activities contribute to overall energy consumption. Through this type of power consumption profiling, developers are able to identify inefficiencies and optimize software to improve energy efficiency. This makes it very important for extending battery life in mobile devices, reducing operating costs in data centers, and also minimizing environmental impact through energy consumption. Energy profiling is a step toward making informed decisions to develop more sustainable and cost-effective computing solutions.

Energy profiling bridges the gap between traditional performance metrics and energy consumption, offering developers critical insights into how their software impacts system efficiency.
When programming, developers usually consider the time it takes to complete a program, or the response time from client to server, or the amount of memory it uses. Most don't have an idea of how much energy their program consumes, or how much it can consume in certain cases, and getting this idea is not as trivial as it seems. To get this awareness, measurements could be made, but they are also difficult to get compared to measuring the time a program takes, which is checking the differences in timestamps, or understanding the memory usage. As for measuring energy, the same program can produce different values due to its non-deterministic nature, meaning that results are often expressed as a range of possible values. Also, reducing the execution time of a program does not guarantee that power consumption will follow~\cite{10.1145/3136014.3136031}. To obtain the measurements it is necessary to use software based tools that can facilitate this process, sometimes at the cost of less accurate readings or hardware devices for accurate values.

To perform hardware-based measurements, a power monitor or power meter device~\cite{hackenberg2013power,ge2009powerpack} must be used to obtain the precise values that a system uses when connected to the electrical grid. These devices measure the power drawn from the grid to the machine, but they usually measure the power consumed by the entire system rather than by specific pieces of hardware, which means they have low granularity. Achieving granular measurements, such as isolating power consumption for specific components like the CPU or RAM, requires a more complex setup to avoid reading unnecessary power consumption. This approach is not suitable for developers who only want to analyze the energy performance of their programs, which shows the difficulty of measuring energy consumption.

An alternative with minimal overhead is to use software-based tools. These tools are typically easier to use, but may not provide values as accurate as hardware-based measurements. However, they are more versatile and can be implemented or modified to meet the user's needs. 


\section{Energy Tools} \label{sec:background_energy}

Understanding and optimizing energy consumption in software requires specialized tools capable of measuring and analyzing power usage. These tools provide valuable insights into how programs consume energy during execution, enabling developers to identify inefficiencies and optimize performance.

At the start some tools were tested in order to see the one that best suited the needs of the project:

Intel RAPL (Running Average Power Limit)~\cite{intel_rapl} is a tool for monitoring power consumption. It utilizes Model-Specific Registers (MSRs), which are used for program execution tracing, performance monitoring, and toggling CPU features. These registers can store the total energy usage of the CPU and memory, allowing it to be read and analyzed. Most software based tools rely on RAPL to measure energy consumption, since it is pretty accurate and is widely available in most CPUs.


Perf~\cite{perfwiki_main} is a Linux tool primarily designed for analyzing application performance characteristics rather than precise energy measurement. While it can provide some energy-related metrics, its measurements tend to be imprecise. In this context, Perf was used mainly to get a rough idea of energy consumption and to serve as an alternative when more accurate tools were unavailable.

Powertop~\cite{archlinux_powertop} was also tested, but it could only perform energy measurements on laptops, as it relies on battery drain data to calculate energy consumption. Since this approach does not align with our specific requirements, we considered Powertop as a last-resort option.

JoularJx~\cite{noureddine-ie-2022} is an energy measurement tool capable of measuring the consumption of Java programs and its methods. However, it is not as precise as other tools as it requires to measure the entire start of the JVM and whole functions instead of small code blocks.


PowerJoular~\cite{noureddine-ie-2022} is an open source tool, capable of measuring energy, from the CPU and GPU, using the Intel RAPL power data through the Linux powercap interface it can read the energy from the CPU and for the GPU it uses NVIDIA SMI to directly read the power consumption.
To read the power consumption of specific processes, PowerJoular monitors the CPU cycles and utilization of each process. By knowing the total power consumption of the CPU through the RAPL interface, it can calculate the power usage of individual processes based on their CPU utilization.
It is build in ADA, that is considered one of most energy efficient programming languages~\cite{PEREIRA2021102609}, and it can monitor applications by name or PID. In this work, it will be necessary to measure the energy consumption of programs, methods, and specific code snippets. To achieve this, PowerJoular will be employed as the primary tool for energy profiling.

Experiment-Runner~\cite{S2_Group_Experiment_Runner} is a framework built in python made to facilitate experiments, it is easily customizable and can be used with energy measurement tools, such as PowerJoular, to monitor the power consumption of another process. While it offers robust support for energy-related experiments, some issues were identified during its use, which will be discussed in detail in Section~\ref{chapter:results}.

\begin{table}[H]%[htbp]
\centering
\caption{Comparison of energy measurement tools}
\begin{tabular}{|p{2.8cm}|p{5.4cm}|p{5.4cm}|}
\hline
\textbf{Tool} & \textbf{Pros} & \textbf{Cons} \\
\hline
\textbf{Intel RAPL}~\cite{intel_rapl} & 
- Hardware-level accuracy \newline
- Widely supported in modern CPUs \newline
- Low overhead and high precision & 
- Not easily accessible \newline
- Cannot measure per-process energy directly \\
\hline
\textbf{Perf}~\cite{perfwiki_main} & 
- Standard Linux tool \newline
- Useful for performance + rough energy metrics & 
- Not designed for energy profiling \newline
- Low precision in energy consumption \\
\hline
\textbf{Powertop}~\cite{archlinux_powertop} & 
- Easy to use \newline
- Good for overall system power stats on laptops & 
- Relies on battery drain \newline
- Ineffective on desktops or servers \\
\hline
\textbf{JoularJx}~\cite{noureddine-ie-2022} & 
- Measures Java program energy \newline
- Method-level granularity possible & 
- Less precise \newline
- Only works with Java \newline
- Can only measure large code blocks \\
\hline
\textbf{PowerJoular}~\cite{noureddine-ie-2022} & 
- Support for CPU and GPU \newline
- Supports per-process measurement \newline
- Built in energy-efficient ADA \newline
- Monitors by name or PID & 
- Multi-thread measurement is limited \newline
- Slightly more complex setup \\
\hline
\textbf{Experiment Runner}~\cite{S2_Group_Experiment_Runner} & 
- Highly customizable \newline
- Integrates well with PowerJoular \newline
- Ideal for repeatable experiments & 
- Limited control over measurement internals, since it relies on a prebuilt framework instead of a custom solution \newline
- Some bugs and issues in practical use \\
\hline
\end{tabular}
\label{tab:tool_comparison}
\end{table}



\section{Code static and dynamic analysis} \label{sec:background_static_dynamic_analysis}

Static analysis, as the name implies, is the process of analyzing the code statically, meaning it examines the code without executing it. By examining the code, tools that leverage static analysis tools can understand how the program will behave at runtime~\cite{ernst2003static}, this analysis often aims for soundness, meaning that if the tool catches an error, it means that the error really exists, there are no false negatives. However, this can come at the cost of producing false positives, where issues that are not actually problems are flagged, so it's important to keeps a balance between them. This analysis allows to check the entire source code and every path, much like compilers check syntax and types. Still, they can only predict some behaviors, as some can only be found when the program is executed, for example, by using dynamic analysis.

Dynamic Analysis is the process of analyzing a program while it is running, allowing for real-time observation of its behavior.  This type of analysis leaves no doubt about memory usage, output, the path taken, how much time it took, among many other factors that are only clear when executing the program~\cite{ernst2003static}. A good example of dynamic analysis is unit testing, which tries to cover as many code paths as possible with different inputs, to understand as much as possible how the program works, and to find something that might be difficult to find with static analysis. However, because its nature, dynamic analysis can be time-consuming, it requires executing the program, which can be slow, especially for large applications or complex codebases. It also requires a lot of resources, such as memory and processing power, and it is not always possible to execute the program in all scenarios. For example, some programs may require specific hardware or software configurations that are not available in all environments. Additionally, dynamic analysis can only analyze the code that is executed during the run, which means that some parts of the code may not be covered by the analysis.
For fast power estimation, static analysis is often preferable due to its speed and scalability. While it may be less accurate than dynamic analysis, it remains a viable option, particularly for large codebases with many dependencies. Static analysis avoids the need for code execution and is generally less resource-intensive, as discussed by Ernst in the context of program analysis trade-offs~\cite{ernst2003static}.
In addition, static analysis is more portable because its setup is much simpler than the more complex setup required for dynamic analysis.
Developers may not have the time or the infrastructure to run the program just to get an average measure of energy consumption for a code snippet or program. Therefore, using static analysis to infer energy consumption makes sense in this context.

The use of static analysis implies the use of a parsing technique. This technique involves analyzing the syntactic structure of the provided code, respecting the rules of the language in which it is written. First, it is necessary to perform a lexical analysis to obtain the keywords, identifiers and tokens that the language contains. Then the parser uses these tokens together with the grammar rules of the programming language and outputs a tree. The output is an Abstract Syntax Tree (AST), which contains the logical structure of the code and allows further analysis. In the context of this work, this technique allows analyzing, for example, Java code and obtain its structure to find out which statements have been used.


The tool proposed in this work will primarily rely on static analysis to achieve its objectives. Static analysis, by examining the code without executing it, is particularly effective in providing early insights into potential energy inefficiencies during the development process. By evaluating all possible code paths and scenarios, it avoids the dependence on specific runtime conditions inherent in dynamic analysis, making it a powerful tool for identifying and addressing energy-related issues without the complexity of real-time monitoring.

However, to create the energy consumption dataset required to train machine learning models, this work will utilize dynamic analysis. This approach enables the collection of accurate and context-aware energy consumption measurements, which are crucial for building a reliable dataset to inform and enhance the energy optimization models. By combining the strengths of both static and dynamic analysis, this work aims to develop a comprehensive framework for energy-efficient software design.


\section{Language Server Protocol} \label{sec:background_lsp}

The Language Server Protocol (LSP) is an open protocol developed by Microsoft for separating language logic (such as code completion, diagnostic information, and symbol resolution) from the editor or development environment on which they are run. The objective of LSP is to allow programming language tools to be reused~\cite{10.1145/3550355.3552452} in different code editors to gain better portability while also reducing the work required to support different environments.

The LSP runs on top of a client-server architecture. The language server, which is usually implemented in the language being analyzed, provides a number of features, such as syntax highlighting, error detection, semantic analysis, and navigation in the source. The client, usually embedded in the editor, talks over the JSON-RPC-based protocol to the server~\cite{lsp}.

With LSP, language-specific behavior can be integrated in multiple editors without reimplementation of the underlying logic for each editor. For example, while in-plugin configuring of an environment such as IntelliJ IDEA or Eclipse generally requires deep integration of the in-plugin code with the specific platform’s native APIs, an LSP implementation eliminates such complexity. If the editor is LSP-compatible, something which is the case for most modern editors, the editor is then able to operate with the language server without further configuration. 

Within the context of the work, LSP is employed as the main technology for the development of a cross-platform language analysis backend. The language server is developed in Java, encapsulating the static analysis and model interaction logic, while the frontend is developed in web technologies (TypeScript and HTML) to provide the user interface within Visual Studio Code. 

While LSP provides cross-editor support for language-related features, it does not account for UI integration into editors. For example, VSCode supports custom interfaces like WebView APIs, which allow extensions to show HTML and TypeScript information in the editor. However, other IDEs like IntelliJ IDEA or Eclipse use different UI integration schemes and don't have web-based interfaces supported in the same way. Therefore, while the language analysis component of the extension can be reused in editors supporting LSP, the UI may need to be redesigned or adapted for each environment.


\section{Machine Learning} \label{sec:background_machine_learning}

Using a machine learning (ML) model to estimate energy consumption can offer advantages over a traditional approach. While traditional approaches such as empirical estimates based on historical data may work, they may not be the best solution in this case due to the unpredictable behavior of energy. Using an ML algorithm can help identify more complex patterns and provide a highly accurate estimate while adapting to the arrival of new information.

To predict energy, an ML model will be used. It's important to understand how different ML algorithms work and which ones are best suited for the proposed project. There are several ML algorithms, and they fall into four main categories~\cite{sarker2021machine}: supervised learning, unsupervised learning, semi-supervised learning and reinforcement learning.

The supervised ML requires labeled data for the model to train on. During training, the model has access to the input and output parameters, and it will try to match inputs to the correct outputs. It has two categories: classification, where it predicts discrete labels, such as whether a picture is a cat or a dog; and regression, where it predicts continuous values, for example, predicting the price of a house based on location, size, and so on, or, as in our project, predicting energy consumption. These models can be very accurate, but they can also make incorrect predictions for patterns they were not trained on. These techniques also are accompanied by some metrics to evaluate how well the models perform.



For \textit{classification models}, some of the most important metrics include:
\begin{itemize}
    \item \textbf{Accuracy:} Measures the percentage of correctly classified instances.
    \item \textbf{Precision:} Focuses on the correctness of positive predictions.
    \item \textbf{Recall (Sensitivity):} Evaluates how well the model detects actual positives.
    \item \textbf{F1 Score}: Balances precision and recall, useful when data is imbalanced.
    \item \textbf{ROC-AUC (Receiver Operating Characteristic - Area Under Curve):} Assesses classification performance across different thresholds.
\end{itemize}

On the other hand, some of the most important metrics for \textit{regression models} are:
\begin{itemize}
    \item \textbf{R² (R-squared or Coefficient of Determination):} Measures how well the model's predictions fit the actual data. A value close to 1 indicates a better fit.
    \item \textbf{MSE (Mean Squared Error):} Computes the average squared difference between predicted and actual values, penalizing large errors more heavily.
    \item \textbf{RMSE (Root Mean Squared Error):} The square root of MSE, offering a more interpretable error magnitude.
    \item \textbf{MAE (Mean Absolute Error):} Calculates the average absolute difference between predictions and actual values, making it less sensitive to extreme outliers.
\end{itemize}


In unsupervised ML, the model attempts to find patterns and relationships in the unlabeled data set. With this technique, it is possible to find similarities or clusters in the data, for example, to detect an anomaly in the data. This technique does not require the effort of acquiring labeled data, but also it will be harder to understand if the output is correct or not.

Semi-supervised learning, as the name implies, uses a combination of supervised and unsupervised learning techniques. This hybrid approach is very useful in real-world scenarios where parts of the data can be labeled and others can't, allowing for better predictions in the output.

Reinforcement learning, where the model receives different feedback for different tasks and uses this feedback to perform the tasks in the most optimal way. The feedback can be in the form of rewards or penalties so that the model can better understand whether it is doing the task correctly. For example, a model playing a video game is rewarded for completing levels faster and penalized for failing the level. This method of learning allows for complex solutions to sequence-based problems, such as robotics or gaming. However, it can be time-consuming and computationally expensive.

There are some algorithms that can meet the proposed model requirements, such as: Linear regression, Tree-Based Models (e.g., Random Forest, Gradient Boosting), Neural Networks, Gaussian Processes, Support Vector Machines (SVM) and or leveraging Genetic Programming.

The most common approach in ML is to use a linear regression algorithm, which is the simpler to implement and very effective. It can predict a continuous output based on the input independent variables. Linear regression is computationally efficient, easy to implement, and works well when the relationship between the input features and the output is linear. However, its simplicity is also its limitation, as it struggles with nonlinear relationships and can underperform when the input features interact in complex ways. It is also sensitive to outliers, which can significantly skew the results.

Tree-based models rely on decision tree models, which are used for structuring decisions, where in each branch a decision is made based on some criteria, and the end of the branch contains the final output. Random forest is a tree-based model that combines multiple decision trees to build an accurate model. When a prediction is needed, all the decision trees provide a vote, in classification or an average in regression and the random forest combines them to give the final prediction. Each tree is trained in different subsets of the data. This algorithm provides high accuracy, robustness to overfitting and estimates the features' importance, however, it has a higher computational cost, more memory usage, and it can take more time to reach a prediction than other approaches.

Gradient boosting is also a tree-based model, it builds trees (weak learners) sequentially with each of them trying to correct the errors of the previous one. It starts with a simple model and iteratively adds trees to reduce residual errors from the previous trees. It is generally more accurate than a random forest, however it is more prone to overfitting if there is a lot of noise in the data.

Neural networks models are particularly good at capturing complex, nonlinear relationships between inputs and outputs. A neural network consists of an input layer, hidden layers, and an output layer. The input layer receives the features (e.g., program attributes derived from the AST), the hidden layers process these features using weights, biases, and activation functions, and the output layer provides the final prediction. Neural networks are highly flexible and can adapt to various problem domains, automatically learning feature representations without requiring extensive manual engineering. However, they require large datasets to avoid overfitting and significant computational resources for training.

Gaussian Processes are particularly interesting, because they take into account the probabilistic nature of energy measurement and provide a range of possible values alongside with the probabilities. This makes them ideal for tasks where understanding the uncertainty in predictions is important, such as energy modeling. However, their computational complexity grows significantly with the size of the dataset, making them less practical for large-scale problems.

Support Vector Machines (SVMs) are a powerful tool in machine learning, capable of performing both classification and regression tasks. This algorithm identifies the optimal hyperplane in an N-dimensional space where it can separate all the features. When it is difficult to separate the features, a technique called kernel trick can be used to create an additional dimension to help separate them. This makes them good for high dimensional spaces, the ability to handle nonlinear relationships and the ability to ignore outliers. However, it can be harder to train this model, and tune the parameters.

Another alternative is to use genetic programming, which is not exactly a conventional ML algorithm, but rather a technique that can be applied to solve ML problems. It is a form of artificial intelligence inspired by the process of natural selection and evolution, where potential solutions to a problem are represented as programs or symbolic expressions. These programs improve iteratively, over generations, through mutations and crossovers, that sometimes can be random, guided by a fitness function. Genetic programming is useful for tasks like symbolic regression and feature classification. However, it can be computationally expensive and can produce inconsistent results due to its uncertain nature.

A good example of genetic programming in machine learning is Python Symbolic Regression (PySR). PySR builds on this technique to discover mathematical expressions that model data in a way that is both accurate and interpretable. Unlike black-box models, PySR generates human-readable formulas, making the results more interpretable and transparent.
PySR works by searching the space of mathematical expressions to find those that best fit the data:

\begin{itemize}
    \item It starts with a random population of simple mathematical expressions, such as $x + 1$ or $\sin(x)$.
    \item Evaluates how well each expression fits the data using a loss function (like MSE).
    \item Expressions are then evolved over several generations using operations like mutation and crossover to produce new, often more complex formulas.
\end{itemize}

At the end of the process, PySR provides a list of candidate expressions, typically sorted along a Pareto front balancing complexity and accuracy. While more complex expressions often provide better predictions, users can select simpler ones if interpretability is a priority.

This technique is particularly well-suited for energy prediction, as it can reveal easy-to-understand formulas that explain how certain parts of a program contribute to energy consumption. This can help developers understand and optimize the energy efficiency of their code more effectively than with traditional black-box models.

These algorithms were taken into account when developing the model that can best predict energy.
Specifically, the models evaluated in this project include: Decision Tree Regressor, Random Forest, Gradient Boosting, Linear Regression, and PySR. These algorithms were chosen based on characteristics such as the simplicity of linear regression, the predictive strength of trees, and the interpretability of symbolic regression with PySR. Together, these models offer a balance of accuracy, efficiency, and explainability, which is important for modeling energy consumption in Java applications.


