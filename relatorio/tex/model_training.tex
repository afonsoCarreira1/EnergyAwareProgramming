\section{Stage 3: Model Training} \label{sec:work_stage3_model_training}



First step to start training the machine learning models it is to extract features from the energy profiles. Features are the characteristics of the code that will be used to train the models and are based on the code structure and behavior. The features extracted can be seen in Table~\ref{tab:features_extracted}.

\begin{table}[htbp]
\centering
\caption{Features Extracted}
\label{tab:features_extracted}
%\makebox[\textwidth][c]{% uncomment if needed and comment line bellow
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{l|c}
\hline
\textbf{Feature Name} & \textbf{Description} \\
\hline\hline
VariableDeclarations & Number of variable declarations in the code. \\
\hline
Assignments & Number of assignment operations (=) in the code. \\
\hline
BinaryOperators & Number of binary operators used (e.g., +, -, *, /, \&\&). \\
\hline
MethodInvocations & Number of method calls in the code. \\
\hline
CustomMethodsUsed & Number of user-defined (custom) methods used in the code. \\
\hline
CustomObjectsUsed & Number of user-defined (custom) objects used in the code. \\
\hline
BranchCount & Number of branching statements ( if, else if, else ) in the code. \\
\hline
LoopCount & Number of loops ( for, while, do-while ) in the code. \\
\hline
LoopMaxDepth & Maximum nesting depth of loops in the code. \\
\hline
CyclomaticComplexity & The cyclomatic complexity of the code (number of independent execution paths). \\
\hline
LiteralCount & Number of literals (numbers, strings, boolean values) in the code. \\
\hline
VariableCount & Total number of variables in the code. \\
\hline
Reassignments & Number of times variables are reassigned a new value. \\
\hline
VariableTypes & Types of variables used (e.g., int, String, List$<$String$>$). \\
\hline
ImportsUsed & List of imported Java packages/libraries in the code. \\
\hline
java.langMethodUsed & If the method is from java.lang, the specific function name (e.g., ArrayList.get). \\
\hline
Inputs & The parameters received by the method. \\
\hline\hline
\end{tabular}}
\end{table}



There are a lot of possible ways of using machine learning, however in this case the approach that best fit our need is supervised machine learning. So, some models were trained to see how good they performed using the data collected.

 

On the context of this project, the first step it is to merge the features to start training the models. While each method is initially trained individually, it is not useful to treat \texttt{LinkedList.add(Object)} and \texttt{ArrayList.add(Object)} as entirely separate cases. Both represent the same \texttt{List.add(Object)} method, differing only in their characteristics specific to their implementation. Therefore, once all energy profiles have been collected, a merging step is performed to consolidate these related methods. All these merged features are stored in a new CSV and for the python program in a Data frame, to be processed.

To implement this functionality, Python scripts were used, combining libraries specialized on machine learning, like scikit-learn (sklearn)~\cite{scikit-learn} which contains models and functions to evaluate the models, and PySR~\cite{cranmer2023interpretablemachinelearningscience} (detailed in Section~\ref{sec:background_machine_learning}).
%which was already explained in Section~\ref{sec:background_machine_learning} an is also already implemented.

Among the available models, we selected a subset of them to train: Decision Tree Regressor, Random Forest, Gradient Boosting, Linear Regression, and PySR. Firstly the values of the MSE and R² are evaluated by the default values of sklearn. GridSearch was used to find the best parameters for a model. The GridSearch does not work with PySR as they are from different libraries so, the parameters for PySR were manually set. When the models are trained, they are moved to the extension folders ready to be used, and all the metrics for each model are logged to a file for subsequent analysis.


\begin{comment}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{figures/r2_comparison.pdf}
  \caption{R² Comparison (Higher is better)}
  \label{fig:r2_comparison}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{figures/mse_comparison.pdf}
  \caption{MSE Comparison (Lower is better)}
  \label{fig:mse_comparison}
\end{figure}

The results of the R² can be seen in the Figure~\ref{fig:r2_comparison}.
Ideally the best score is 1, meaning that the model can get 100\% of the prediction right, however that is not possible in most cases, and this project is not an exception to the rule. It is noticeable that most values are really low (bellow 0.8), which means that the model cannot predict the energy very well for some methods. The best models were for the method \texttt{addAll()} and \texttt{containsAll()} of the List collection, which got an R² of around .8 for most models, meaning that for those 3 methods on average the predictions will be 80\% correct.

For the other methods the scores were lower. The explanation found for this result was the following:


\begin{figure}[htbp]
  \centering
  \includegraphics[width = .8 \textwidth]{figures/size_energy.pdf}
  \caption{Energy for the \texttt{List.size()} method with different list sizes.}
  \label{fig:size_energy}
\end{figure}

The 3 methods with the most accuracy were the ones that generated bigger energy outputs, because the methods were computationally more powerful than the others. This made it so that bigger inputs would result in bigger energy outputs, and make the models more easily find a pattern to predict the energy. As for the other models, since a bigger input would not mean a bigger energy output, the model, might had some difficulties predicting the energy. This does not mean that a lower R² will completely make the model unusable for some methods, as their MSE, that can be seen in the Figure~\ref{fig:mse_comparison} is not high, meaning that even when failing to predict the energy of the method, the failed prediction will not be far away as one might expect. For example, if a model has an MSE of $1 \times 10^{-10}$ and predicts the energy usage to be $1~\mathrm{J}$, then even if the prediction is not exact, the true value is likely within $\pm \sqrt{1 \times 10^{-10}} = \pm 1 \times 10^{-5}~\mathrm{J}$ of the prediction, indicating accurate performance despite a potentially low R². 

Another factor that may contribute to low R² scores is the behavior of the energy measurement tool when applied to certain methods. For instance, some methods, such as \texttt{size()}, do not require more computational effort as the collection size increases. Whether the collection has one element or one million, the method executes in roughly the same amount of time. Consequently, the energy consumption remains nearly constant, regardless of the input size. Since these methods complete very quickly and consume very little energy, even minimal measurement noise can significantly affect the recorded values. Figure~\ref{fig:size_energy} illustrates this effect: although an increase in energy with input size is typically expected, the recorded energy values for \texttt{List.size()} remain considerably flat. While this example isolates a single feature, and other features also influence energy consumption, input size is often the most significant. This observation suggests that for low-energy, low-variance methods, measurement noise can impact the signal, making accurate energy prediction particularly challenging.


Nonetheless, the lower R² scores should be addressed, and what can be done to improve the results of these models is to have the program generator create higher inputs, so the energy profiles also have outputs with higher energy, making the energy predictions more accurate. Then, since most of the predictions depend on the input, it means the other features do not have such higher impact, so it is also important to pick better features and remove others that might not be interesting. 



In general, most of the models present similar scores, however the chosen one was PySR. As it can represent the predictions in expressions which can help the users to try to understand why the code is using more energy. It has a nice feature of allowing to balance complexity and accuracy. And can easily be used in another code language as it is represented as a mathematical expression.
The fact that most models present similar results, ranging from advanced models like Random Forest and Gradient Boosting to simple models like Linear Regression, suggests that features used in the model likely do not capture highly nonlinear or complex relations. This indicates that energy consumption behavior in analyzed approaches can be reliably captured by simple relations. As a result, the set of features cannot offer the richness or variability needed for distinguishing more subtle energy behavior. This does not mean that the learning problem is simple, rather, it indicates that the available features may fail to express potential nonlinear patterns of energy consumption. To improve prediction performance and enable more effective use of expressive models, future work should incorporate features capable of highlight the complexity of predicting energy usage.



\end{comment}